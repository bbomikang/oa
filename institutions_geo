import os
import json
import pandas as pd
from datetime import datetime

# Main directory containing subfolders with JSON files
main_directory_path = '/cwork/bk203/openalex-snapshot/data/institutions'

# Function to process each file (handles JSON Lines format)
def process_file(file_path, output_directory, updated_date, log_file):
    log_file.write(f"[{datetime.now()}] Processing file: {file_path}\n")

    # List to store processed data
    structured_data = []

    # Open the file and read it line by line 
    try:
        with open(file_path, 'r') as file:
            for line in file:
                try:
                    data = json.loads(line)  # Parse each line as an individual JSON object
                    # Extract relevant fields, providing default values if keys are missing
                    geo_data = data.get("geo", {})
                    structured_entry = {
                        "id": data.get("id"),
                        "city": geo_data.get("city"),
                        "region": geo_data.get("region"),
                        "country": geo_data.get("country"),
                        "latitude": geo_data.get("latitude"),
                        "longitude": geo_data.get("longitude")
                    }
                    structured_data.append(structured_entry)
                except json.JSONDecodeError:
                    log_file.write(f"[{datetime.now()}] Skipping invalid JSON in {file_path}\n")
    except (OSError, IOError) as e:
        log_file.write(f"[{datetime.now()}] Error reading file {file_path}: {e}\n")
        return

    # Convert to pandas DataFrame
    log_file.write(f"[{datetime.now()}] Converting to DataFrame...\n")
    df = pd.DataFrame(structured_data)

    # Create the cleaned file name, appending the updated date
    original_file_name = os.path.basename(file_path)
    cleaned_file_name = f"{original_file_name.split('.')[0]}_{updated_date}.csv"
    output_file = os.path.join(output_directory, cleaned_file_name)

    # Save to CSV
    df.to_csv(output_file, index=False)
    log_file.write(f"[{datetime.now()}] Finished processing file: {file_path}\n")

# Main function to iterate over all subfolders and files
def process_all_files_in_subfolders(main_directory_path, output_directory):
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Open log file
    log_file_path = os.path.join(output_directory, 'processing_log.txt')
    with open(log_file_path, 'a') as log_file:
        # Iterate through each subfolder in the main directory
        for subfolder_name in os.listdir(main_directory_path):
            subfolder_path = os.path.join(main_directory_path, subfolder_name)

            # Check if it's a directory (ignores files)
            if os.path.isdir(subfolder_path) and subfolder_name.startswith('updated_date='):
                updated_date = subfolder_name.split('=')[1]  # Extract the updated date
                log_file.write(f"[{datetime.now()}] Processing subfolder: {subfolder_name}\n")

                # List files in the subfolder for debugging
                files_in_subfolder = [f for f in os.listdir(subfolder_path) if f.startswith('part_')]
                log_file.write(f"[{datetime.now()}] Files in subfolder {subfolder_name}: {files_in_subfolder}\n")

                for filename in files_in_subfolder:
                    file_path = os.path.join(subfolder_path, filename)
                    process_file(file_path, output_directory, updated_date, log_file)

# Function to append all CSV files into one DataFrame and save
def append_all_csv_files(output_directory, final_csv_path):
    print(f"[{datetime.now()}] Appending all CSV files in {output_directory}...")
    all_files = [os.path.join(output_directory, f) for f in os.listdir(output_directory) if f.endswith('.csv')]
    df_list = []
    for file in all_files:
        df = pd.read_csv(file)
        df_list.append(df)
    final_df = pd.concat(df_list, ignore_index=True)
    final_df.to_csv(final_csv_path, index=False)
    print(f"[{datetime.now()}] Finished appending all files into {final_csv_path}")

# Define the output directory
output_directory = '/cwork/bk203/cleaned_files_institutions'
final_csv_path = '/cwork/bk203/cleaned_files_institutions/final_combined.csv'

# Run the batch processing for all files in all subfolders
process_all_files_in_subfolders(main_directory_path, output_directory)

# Append all CSV files into one
append_all_csv_files(output_directory, final_csv_path)
