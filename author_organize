import os
import json
import pandas as pd
from datetime import datetime

# Main directory containing subfolders with JSON files
main_directory_path = '/cwork/bk203/openalex-sanpshot/data/authors'

# Function to process each file (handles JSON Lines format)
def process_file(file_path, output_directory, updated_date):
    print(f"[{datetime.now()}] Processing file: {file_path}")

    # List to store processed data
    structured_data = []

    # Open the file and read it line by line 
    try:
        with open(file_path, 'r') as file:
            for line in file:
                try:
                    data = json.loads(line)  # Parse each line as an individual JSON object
                    # Extract relevant fields, providing default values if keys are missing
                    structured_entry = {
                        "id": data.get("id"),
                        "display_name": data.get("display_name"),
                        "works_count": data.get("works_count", 0),
                        "cited_by_count": data.get("cited_by_count", 0),
                        "counts_by_year": json.dumps(data.get("counts_by_year", []), ensure_ascii=False),
                        "summary_stats": json.dumps(data.get("summary_stats", {}), ensure_ascii=False),
                        "affiliations": json.dumps(data.get("affiliations", []), ensure_ascii=False),
                        "concepts": ", ".join([concept.get("display_name", "") for concept in data.get("x_concepts", [])])
                    }
                    structured_data.append(structured_entry)
                except json.JSONDecodeError:
                    print(f"[{datetime.now()}] Skipping invalid JSON in {file_path}")
    except (OSError, IOError) as e:
        print(f"[{datetime.now()}] Error reading file {file_path}: {e}")
        return

    # Convert to pandas DataFrame
    print(f"[{datetime.now()}] Converting to DataFrame...")
    df = pd.DataFrame(structured_data)

    # Create the cleaned file name, appending the updated date
    original_file_name = os.path.basename(file_path)
    cleaned_file_name = f"{original_file_name.split('.')[0]}_{updated_date}.csv"
    output_file = os.path.join(output_directory, cleaned_file_name)

    # Save to CSV
    df.to_csv(output_file, index=False)
    print(f"[{datetime.now()}] Finished processing file: {file_path}")

# Main function to iterate over all subfolders and files
def process_all_files_in_subfolders(main_directory_path, output_directory):
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)

    # Iterate through each subfolder in the main directory
    for subfolder_name in os.listdir(main_directory_path):
        subfolder_path = os.path.join(main_directory_path, subfolder_name)

        # Check if it's a directory (ignores files)
        if os.path.isdir(subfolder_path) and subfolder_name.startswith('updated_date='):
            updated_date = subfolder_name.split('=')[1]  # Extract the updated date
            print(f"[{datetime.now()}] Processing subfolder: {subfolder_name}")

            # List files in the subfolder for debugging
            files_in_subfolder = [f for f in os.listdir(subfolder_path) if f.startswith('part_')]
            print(f"[{datetime.now()}] Files in subfolder {subfolder_name}: {files_in_subfolder}")

            for filename in files_in_subfolder:
                file_path = os.path.join(subfolder_path, filename)
                process_file(file_path, output_directory, updated_date)

# Define the output directory
output_directory = '/cwork/bk203/cleaned_files_authors'

# Run the batch processing for all files in all subfolders
process_all_files_in_subfolders(main_directory_path, output_directory)
